---
title: 'O2O-Afford: Annotation-Free Large-Scale Object-Object Affordance Learning'
section: Contributed Papers
openreview: EougVeukEH9
website: https://cs.stanford.edu/~kaichun/o2oafford/
abstract: Contrary to the vast literature in modeling, perceiving, and understanding
  agent-object (e.g., human-object, hand-object, robot-object) interaction in computer
  vision and robotics, very few past works have studied the task of object-object
  interaction, which also plays an important role in robotic manipulation and planning
  tasks. There is a rich space of object-object interaction scenarios in our daily
  life, such as placing an object on a messy tabletop, fitting an object inside a
  drawer, pushing an object using a tool, etc. In this paper, we propose a unified
  affordance learning framework to learn object-object interaction for various tasks.
  By constructing four object-object interaction task environments using physical
  simulation (SAPIEN) and thousands of ShapeNet models with rich geometric diversity,
  we are able to conduct large-scale object-object affordance learning without the
  need for human annotations or demonstrations. At the core of technical contribution,
  we propose an object-kernel point convolution network to reason about detailed interaction
  between two objects. Experiments on large-scale synthetic data and real-world data
  prove the effectiveness of the proposed approach.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: mo22b
month: 0
tex_title: 'O2O-Afford: Annotation-Free Large-Scale Object-Object Affordance Learning'
firstpage: 1666
lastpage: 1677
page: 1666-1677
order: 1666
cycles: false
bibtex_author: Mo, Kaichun and Qin, Yuzhe and Xiang, Fanbo and Su, Hao and Guibas,
  Leonidas
author:
- given: Kaichun
  family: Mo
- given: Yuzhe
  family: Qin
- given: Fanbo
  family: Xiang
- given: Hao
  family: Su
- given: Leonidas
  family: Guibas
date: 2022-01-11
address:
container-title: Proceedings of The 5th Conference on Robot Learning
volume: '164'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 1
  - 11
pdf: https://proceedings.mlr.press/v164/mo22b/mo22b.pdf
extras:
- label: Supplementary ZIP
  link: https://proceedings.mlr.press/v164/mo22b/mo22b-supp.zip
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
